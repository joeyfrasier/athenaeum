# Athenaeum

**A temple of knowledge for AI agents** - Production-ready agent system based on TigerData's open-source architecture, featuring exactly-once event processing, PostgreSQL-backed durability, and modular MCP server integration.

> Named after Minerva's temple, historically representing a place of learning and intellectual pursuit.

## Overview

This system implements a Slack-native AI assistant with enterprise-grade reliability:

- **Exactly-Once Processing**: PostgreSQL-backed event claiming ensures zero duplicate responses
- **Bounded Concurrency**: Fixed worker pools prevent resource exhaustion
- **Sub-Millisecond Latency**: Immediate event triggering (not polling-based)
- **Horizontal Scalability**: Multiple instances coordinate through PostgreSQL
- **Modular Architecture**: MCP servers provide composable tool integration

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Slack Workspace                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Socket Mode / Events API
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agent Framework (Python)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Worker Pool (Bounded Concurrency)                       â”‚  â”‚
â”‚  â”‚  Event Processor (Atomic Claiming)                       â”‚  â”‚
â”‚  â”‚  Agent Core (Pydantic-AI + Claude)                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PostgreSQL/TimescaleDB (Core Data Layer)           â”‚
â”‚  - Event Queue (Atomic Claiming)                                â”‚
â”‚  - Conversational Memory (Hypertables)                          â”‚
â”‚  - Documentation Embeddings (pgvector)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MCP Server Layer (TypeScript)                â”‚
â”‚  - Slack MCP (Conversation Retrieval)                           â”‚
â”‚  - GitHub MCP (Code Search)                                     â”‚
â”‚  - Docs MCP (Semantic Search)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### 1. Exactly-Once Event Processing

Uses PostgreSQL's `FOR UPDATE SKIP LOCKED` for atomic event claiming:

```sql
UPDATE events
SET status = 'processing', claimed_by = 'worker-1'
WHERE id = (
    SELECT id FROM events
    WHERE status = 'pending'
    ORDER BY created_at ASC
    LIMIT 1
    FOR UPDATE SKIP LOCKED
)
RETURNING *
```

**Result**: Zero duplicate responses, even under high concurrency.

### 2. Automatic Retry with Exponential Backoff

Failed events automatically retry with exponential backoff:

- Retry 1: 10 seconds
- Retry 2: 20 seconds
- Retry 3: 40 seconds
- After max retries: Marked as permanently failed

### 3. Time-Series Optimized Storage

Slack messages stored in TimescaleDB hypertables:

- 7-day chunk partitioning
- Automatic compression after 45 days (5-10x space savings)
- Fast recent-data queries

### 4. Semantic Documentation Search

PostgreSQL with pgvector for semantic search:

- OpenAI embeddings
- Cosine similarity ranking
- Sub-100ms query latency

## Quick Start

### Three Setup Options

**Option 1: Docker Self-Hosted** (Recommended - 100% Open Source) ðŸ³
- TimescaleDB in Docker - same tech as TigerData Cloud
- 100% open-source, no vendor lock-in
- Full control over infrastructure
- Perfect for development and self-hosted production
- Includes pgAdmin web UI (optional)

ðŸ‘‰ **[See Docker Setup Guide](DOCKER_SETUP.md)** for complete instructions

**Option 2: TigerData Cloud** (Managed - Production at Scale) ðŸš€
- Fully managed TimescaleDB optimized for AI agents
- 30-day free trial (no credit card required)
- Automatic backups, scaling, and monitoring
- Enterprise support and 99.99% uptime SLA
- Tiger CLI MCP server for AI assistance

ðŸ‘‰ **[See TigerData Cloud Setup Guide](TIGERDATA_SETUP.md)** for complete instructions

**Option 3: Local PostgreSQL**
- Bare-metal PostgreSQL/TimescaleDB installation
- Maximum customization
- Requires manual configuration

Instructions below are for Option 3 (bare-metal). For Docker or TigerData Cloud, see their respective guides.

---

### Prerequisites

- Python 3.11+
- PostgreSQL 14+ or TimescaleDB (local) OR TigerData account (cloud)
- Docker (optional, for local development)
- Slack workspace with bot permissions
- Anthropic API key

### Installation (Local Development)

1. **Clone the repository**:

```bash
git clone https://github.com/your-org/athenaeum.git
cd athenaeum
```

2. **Install Python dependencies**:

```bash
pip install -r requirements.txt
```

3. **Set up PostgreSQL/TimescaleDB**:

```bash
# Using Docker
docker run -d --name production-agent-db \
  -e POSTGRES_PASSWORD=password \
  -e POSTGRES_DB=tsdb \
  -e POSTGRES_USER=tsdbadmin \
  -p 127.0.0.1:5432:5432 \
  timescale/timescaledb-ha:pg17
```

4. **Run database migrations**:

```bash
psql $DATABASE_URL -f migrations/001_initial_schema.sql
```

5. **Configure environment variables**:

```bash
cp .env.sample .env
# Edit .env with your credentials
```

Required environment variables:

```
DATABASE_URL=postgresql://tsdbadmin:password@localhost:5432/tsdb
SLACK_APP_TOKEN=xapp-your-app-token
SLACK_BOT_TOKEN=xoxb-your-bot-token
ANTHROPIC_API_KEY=sk-ant-your-key
```

6. **Create Slack app**:

- Go to [api.slack.com/apps](https://api.slack.com/apps)
- Create new app from manifest (see `slack-manifest.json`)
- Enable Socket Mode
- Install to workspace
- Copy app token and bot token to `.env`

### Running the Agent

```python
from agent import WorkerPool
from agent.config import Config
from database import init_db

# Initialize configuration
config = Config()
config.validate()

# Initialize database
db = init_db(database_url=config.database_url)
db.create_tables()

# Define event processing function
def process_event(event):
    print(f"Processing event: {event.id}")
    # Your agent logic here

# Start worker pool
pool = WorkerPool(
    size=config.worker_pool_size,
    db_connection=db,
    process_func=process_event,
)

pool.start()
pool.run_forever()  # Blocks until shutdown signal
```

## Project Structure

```
athenaeum/
â”œâ”€â”€ agent/                      # Core agent framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ event_processor.py     # Atomic event claiming
â”‚   â”œâ”€â”€ worker_pool.py         # Bounded concurrency
â”‚   â”œâ”€â”€ agent.py               # Agent logic
â”‚   â””â”€â”€ config.py              # Configuration
â”œâ”€â”€ database/                   # Database layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py              # SQLAlchemy models
â”‚   â”œâ”€â”€ connection.py          # Connection pooling
â”‚   â””â”€â”€ migrations/            # SQL migrations
â”œâ”€â”€ slack/                      # Slack integration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest.py              # Real-time ingestion
â”‚   â”œâ”€â”€ client.py              # Slack API client
â”‚   â””â”€â”€ socket_handler.py      # Socket Mode
â”œâ”€â”€ mcp/                        # MCP servers
â”‚   â”œâ”€â”€ slack_mcp/             # Slack MCP
â”‚   â”œâ”€â”€ github_mcp/            # GitHub MCP
â”‚   â””â”€â”€ docs_mcp/              # Docs MCP
â”œâ”€â”€ prompts/                    # Jinja2 templates
â”œâ”€â”€ tests/                      # Test suite
â”œâ”€â”€ migrations/                 # SQL migrations
â”œâ”€â”€ docker/                     # Docker configs
â”œâ”€â”€ scripts/                    # Utility scripts
â”œâ”€â”€ ARCHITECTURE_ANALYSIS.md   # Architecture details
â”œâ”€â”€ IMPLEMENTATION_PLAN.md     # Implementation plan
â”œâ”€â”€ TESTING_STRATEGY.md        # Testing strategy
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ package.json               # TypeScript dependencies
â””â”€â”€ README.md                  # This file
```

## Core Components

### 1. Event Processor

Handles atomic event claiming and processing:

```python
from agent import EventProcessor
from database import get_session

with get_session() as session:
    processor = EventProcessor(session, worker_id="worker-1")

    # Claim an event
    event = processor.claim_event()

    if event:
        try:
            # Process event
            process(event)
            processor.complete_event(event)
        except Exception as e:
            processor.fail_event(event, str(e))
```

### 2. Worker Pool

Manages bounded concurrency:

```python
from agent import WorkerPool

pool = WorkerPool(
    size=5,
    db_connection=db,
    process_func=my_process_function,
)

pool.start()

# Get statistics
stats = pool.get_stats()
print(f"Workers running: {stats['workers_running']}")
print(f"Events processed: {stats['total_events_processed']}")

# Graceful shutdown
pool.stop()
```

### 3. Database Connection

Manages connection pooling:

```python
from database import DatabaseConnection

db = DatabaseConnection(
    database_url="postgresql://...",
    pool_size=10,
    max_overflow=20,
)

# Use context manager
with db.session() as session:
    # Your database operations
    pass

# Health check
is_healthy = db.health_check()

# Pool status
status = db.get_pool_status()
```

### 4. Configuration

Environment-based configuration:

```python
from agent.config import Config

config = Config()
config.validate()
config.log_configuration()
```

## Database Schema

### Events Table

Stores events for processing with exactly-once semantics:

```sql
CREATE TABLE events (
    id BIGSERIAL PRIMARY KEY,
    event_type VARCHAR(50) NOT NULL,
    payload JSONB NOT NULL,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    visibility_timeout TIMESTAMPTZ,
    claimed_by VARCHAR(100),
    retry_count INTEGER DEFAULT 0,
    error_message TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    processed_at TIMESTAMPTZ
);
```

### Slack Messages Table

Time-series optimized message storage:

```sql
CREATE TABLE slack_messages (
    channel_id VARCHAR(50) NOT NULL,
    ts VARCHAR(50) NOT NULL,
    user_id VARCHAR(50),
    text TEXT,
    thread_ts VARCHAR(50),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY (channel_id, ts)
);

-- Convert to hypertable (with TimescaleDB)
SELECT create_hypertable('slack_messages', 'created_at',
    chunk_time_interval => INTERVAL '7 days');
```

## Testing

### Unit Tests

```bash
pytest tests/unit/ -v --cov
```

### Integration Tests

```bash
# Start test database
docker-compose -f docker-compose.test.yml up -d

# Run tests
pytest tests/integration/ -v

# Cleanup
docker-compose -f docker-compose.test.yml down
```

### End-to-End Tests

```bash
pytest tests/e2e/ -v -m e2e
```

### Load Tests

```bash
locust -f tests/performance/locustfile.py --users 50 --spawn-rate 10
```

## Deployment

### Docker Compose (Development)

```bash
docker-compose up -d
```

### Kubernetes (Production)

```bash
kubectl apply -f k8s/
```

See `IMPLEMENTATION_PLAN.md` for detailed deployment strategies.

## Monitoring

### Health Check

```python
# Database health
is_db_healthy = db.health_check()

# Worker pool health
is_pool_healthy = pool.health_check()

# Queue statistics
stats = processor.get_queue_stats()
print(f"Pending: {stats.get('pending', 0)}")
print(f"Processing: {stats.get('processing', 0)}")
print(f"Completed: {stats.get('completed', 0)}")
print(f"Failed: {stats.get('failed', 0)}")
```

### Logging

Structured logging with `structlog`:

```python
import structlog

logger = structlog.get_logger(__name__)
logger.info("event_processed", event_id=123, duration=1.5)
```

### Observability

Optional Logfire integration for distributed tracing:

```python
# Set environment variables
LOGFIRE_TOKEN=your-token
LOGFIRE_PROJECT_NAME=production-agent

# Automatic instrumentation
```

## Configuration

All configuration via environment variables (see `.env.sample`):

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection URL | `postgresql://...` |
| `SLACK_APP_TOKEN` | Slack app-level token | Required |
| `SLACK_BOT_TOKEN` | Slack bot OAuth token | Required |
| `ANTHROPIC_API_KEY` | Anthropic API key | Required |
| `WORKER_POOL_SIZE` | Number of workers | `5` |
| `EVENT_VISIBILITY_TIMEOUT` | Event timeout (seconds) | `300` |
| `MAX_RETRY_COUNT` | Max retry attempts | `3` |

## Performance

### Benchmarks

| Metric | Value |
|--------|-------|
| Event claiming latency | < 10ms (p99) |
| Event processing latency | < 2s (p95) |
| System throughput | > 100 events/sec |
| Database query latency | < 50ms (p95) |

### Scalability

- **Horizontal**: Add more worker instances
- **Vertical**: Increase `WORKER_POOL_SIZE`
- **Database**: TimescaleDB native scaling features

## Troubleshooting

### No events being processed

1. Check database connection: `db.health_check()`
2. Check worker pool status: `pool.get_stats()`
3. Check queue depth: `processor.get_queue_depth()`

### Events failing repeatedly

1. Check error messages: `SELECT error_message FROM events WHERE status = 'failed'`
2. Increase `MAX_RETRY_COUNT`
3. Check LLM API connectivity

### High database load

1. Check connection pool: `db.get_pool_status()`
2. Increase `pool_size` or `max_overflow`
3. Enable query logging: `echo=True` in `DatabaseConnection`

## Contributing

1. Fork the repository
2. Create a feature branch
3. Write tests for new functionality
4. Ensure all tests pass: `pytest tests/ -v`
5. Submit a pull request

## Documentation

- **Architecture**: See `ARCHITECTURE_ANALYSIS.md`
- **Implementation Plan**: See `IMPLEMENTATION_PLAN.md`
- **Testing Strategy**: See `TESTING_STRATEGY.md`
- **Docker Setup**: See `DOCKER_SETUP.md` ðŸ³ (Self-hosted with Docker)
- **TigerData Cloud**: See `TIGERDATA_SETUP.md` â˜ï¸ (Managed cloud database)
- **Slack Integration**: See `SLACK_SETUP.md` ðŸ’¬ (Connect to Slack workspace)
- **Agent Setup**: See `AGENT_SETUP.md` ðŸ¤– (Claude AI integration and running the agent)

## Acknowledgments

Based on TigerData's open-source production agent architecture:

- [tiger-agents-for-work](https://github.com/timescale/tiger-agents-for-work)
- [tiger-slack](https://github.com/timescale/tiger-slack)
- [tiger-docs-mcp-server](https://github.com/timescale/tiger-docs-mcp-server)
- [tiger-gh-mcp-server](https://github.com/timescale/tiger-gh-mcp-server)

Blog post: [We Built a Production Agent (and Open-Sourced Everything We Learned)](https://www.tigerdata.com/blog/we-built-production-agent-open-sourced-everything-we-learned)

## License

Apache 2.0 - See `LICENSE` file for details.

## Support

For issues and questions:
- GitHub Issues: [github.com/your-org/athenaeum/issues](https://github.com/your-org/athenaeum/issues)
- Documentation: See `docs/` directory

## Roadmap

- [x] **Phase 1: Foundation** (âœ… Complete)
  - [x] Database layer with event processor
  - [x] Worker pool with bounded concurrency
  - [x] Configuration management
- [x] **Phase 2: Slack Integration** (âœ… Complete)
  - [x] Socket Mode connection
  - [x] Message ingestion
  - [x] Historical import
  - [x] User/channel sync
  - [x] Real-time event processing
- [x] **Phase 3: Agent Core** (âœ… Complete)
  - [x] LLM integration (Claude)
  - [x] Prompt templates (Jinja2)
  - [x] Agent orchestration logic
  - [x] Conversation context retrieval
  - [x] Main entry point (athenaeum.py)
- [ ] Phase 4: MCP Servers
  - [ ] Slack MCP
  - [ ] GitHub MCP
  - [ ] Docs MCP
- [ ] Phase 5: Production Readiness
  - [ ] Observability
  - [ ] Monitoring
  - [ ] Deployment configs
- [ ] Phase 6: Testing & Documentation
  - [ ] Comprehensive tests
  - [ ] User guides
  - [ ] Security audit

## Version

**v0.3.0** - Agent Core Release (2025-11-01)

- **Phase 3 Complete**: Intelligent AI agent with Claude
  - Claude API integration with retry logic
  - Conversation context retrieval from TimescaleDB
  - Jinja2 prompt templating system
  - Agent orchestration and decision loop
  - Main entry point (athenaeum.py)
  - Streaming response support
  - Complete end-to-end flow
- **Phase 2 Features**: Full Slack integration
  - Socket Mode real-time connection
  - Message ingestion to TimescaleDB
  - User/channel metadata sync
  - Historical message import
  - Reaction tracking
- **Phase 1 Features**: Production-grade foundation
  - Event processor with atomic claiming
  - Worker pool with bounded concurrency
  - Database models and migrations
  - Configuration management
- **Deployment Options**:
  - Docker self-hosted setup
  - TigerData Cloud integration
- **Documentation**: 260+ pages across 7 comprehensive guides
